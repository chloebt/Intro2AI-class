## Lecture 9 - Attention in NLP
Teacher: Romain Bielawski (ANITI)

### Contents

* Vanishing gradient
* Attention in LSTM
* Self attention and transformer
* Embedding : Bert
* Generation : GPT
* Application : Story generation


### Prerequisites:
Knowledge about neural networks principles; knowledge of several NN layer types; gradient descent & backpropagation; basics of NLP; RNN principles


### Further reading:

---
#### [(Back to Main Page)](../index.md)
